import torch
import json
import matplotlib.pyplot as plt
import seaborn as sns
from model import SimpleTransformer
import tokenizer

# Configuration
VOCAB_PATH = r"CreatedVocab\vocab.json"
MODEL_PATH = r"TrainedModel\small_transformer.pth"
EMBEDDING_SIZE = 256
NUM_LAYERS = 3
NUM_HEADS = 8
MAX_SEQUENCE_LENGTH = 256
DROPOUT_RATE = 0.1

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load vocabulary
vocab = tokenizer.load_vocab(VOCAB_PATH)

# Load model
model = SimpleTransformer(
    vocab_size=len(vocab),
    embedding_size=EMBEDDING_SIZE,
    num_layers=NUM_LAYERS,
    num_heads=NUM_HEADS,
    max_sequence_length=MAX_SEQUENCE_LENGTH,
    dropout_rate=DROPOUT_RATE
).to(device)

model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
model.eval()

def encode_prompt(prompt):
    input_ids = tokenizer.tokenize(prompt, vocab)
    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)
    return input_ids, input_tensor

# Text Generation 
def generate_text(prompt, max_new_tokens=20):
    input_ids, input_tensor = encode_prompt(prompt)

    with torch.no_grad():
        for _ in range(max_new_tokens):
            output, attentions = model(input_tensor, return_attentions=True)
            next_token = torch.argmax(output[0, -1]).item()
            input_ids.append(next_token)
            input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)

    generated_tokens = tokenizer.detokenize(input_ids, vocab)
    return " ".join(generated_tokens), input_ids, attentions

# Attention Visualization
def visualize_attention(input_ids, attentions):
    tokens = tokenizer.detokenize(input_ids, vocab)
    num_layers = len(attentions)
    num_heads = attentions[0].size(1)
    seq_len = len(tokens)

    fig, axes = plt.subplots(num_layers, num_heads, figsize=(24, 12), constrained_layout=True)

    for layer in range(num_layers):
        for head in range(num_heads):
            ax = axes[layer, head]
            attn = attentions[layer][0][head].cpu().detach().numpy()
            attn_matrix = attn[:seq_len, :seq_len]

            sns.heatmap(attn_matrix,
                        xticklabels=tokens,
                        yticklabels=tokens,
                        cmap="YlGnBu",
                        ax=ax,
                        cbar=False,
                        square=True)

            ax.set_title(f"Layer {layer + 1}, Head {head + 1}", fontsize=10)
            ax.tick_params(axis='x', labelsize=6)
            ax.tick_params(axis='y', labelsize=6)
          

    plt.show()


prompt = "Once upon a time"
prompt = "ROMEO O ROMEO"
generated_text, input_ids, attentions = generate_text(prompt)

print("Given Prompt from User:-",prompt)
print("Text Generated by Trained Model:-", generated_text)

visualize_attention(input_ids, attentions)





